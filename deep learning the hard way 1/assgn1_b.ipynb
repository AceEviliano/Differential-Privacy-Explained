{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A 2 layer shallow network for binary classification\n",
    "In this assignment you will build a two layer network for the same cat vs non-cat binary classification problem. First, lets import the required packages. Note that we have copied the functions 'flatten', 'load_train_data' and 'load_test_data' functions to 'assign1_utils.py'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib # for plotting\n",
    "from matplotlib import pyplot as plt # for plotting\n",
    "\n",
    "from assign1_utils import load_train_data, load_test_data, flatten\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture to be implemented is as follows:\n",
    "Ip layer(I)----->hidden layer(H)------>op layer(O)\n",
    "\n",
    "- Ip features shape: nx$^{[0]}$ x m (a batch of m samples each of dim nx. In this assignment, nx$^{[0]}$ will be 64\\*64*3 = 12288).\n",
    "- weights between I and H have shape: nx$^{[1]}$ x nx$^{[0]}$. nx$^{[1]}$ = 32.\n",
    "- bias vector at H has shape: nx$^{[1]}$ x 1\n",
    "- non-linearity at hidden layer is ReLU\n",
    "- weights between H and O have shape: nx$^{[2]}$ x nx$^{[1]}$. nx$^{[2]}$ = 1.\n",
    "- bias vector at H has shape: nx$^{[2]}$ x 1\n",
    "- non-linearity at output layer is Sigmoid\n",
    "\n",
    "The implementation will follow the python style pseudo-code listed in my lecture notes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you will complete the function that intializes weights and biases and returns them. Weight matrices have to be initialized similar to how weight vector was initialized in logistic regression. Bias vectors have to be initialized to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(nx):\n",
    "    \"\"\"\n",
    "      Function that intializes weights to scaled random std normal values and biases to zero and returns them\n",
    "      \n",
    "      nx: a list that contains number of nodes in each layer in order. For a l-layer network, len(nx) = l+1 \n",
    "          as it includes num of features in input layer also.\n",
    "          \n",
    "      returns W: list of numpy arrays of weight matrices\n",
    "              b: list of numpy arrays of bias vectors\n",
    "    \"\"\"\n",
    "    Wlist = []\n",
    "    blist = []\n",
    "    for i in range(1, len(nx)): \n",
    "        Wlist.append(...) # replace the ...; np.random.randn will be useful\n",
    "        blist.append(...) # replace the ...; np.zeros will be useful\n",
    "    return Wlist, blist\n",
    "\n",
    "\n",
    "#uncomment the following two lines to test your function\n",
    "\n",
    "#W, b = initialize_weights([3, 2, 1])\n",
    "#[print(f'Shape of W[{i}]: {W[i].shape}, Shape of b[{i}]: {b[i].shape}') for i in range(len(W))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will complete forward, backward, update_params and part of the main function. Functions f and df are already comlete. Look at the code to understand what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(z, fname = 'ReLU'):\n",
    "    \"\"\"\n",
    "      computes and returns the non-linear function of z given the non-linearity\n",
    "      \n",
    "      z: numpy array of any shape on which the non-linearity will be applied elementwise\n",
    "      fname: a string that is name of the non-linearity. Defaults to 'ReLU'. Other valid values are\n",
    "             'Sigmoid', 'Tanh', and 'Linear'.\n",
    "      \n",
    "      returns f(z) f is the non-linear function whose name is fname\n",
    "    \"\"\"\n",
    "    if fname == 'ReLU':\n",
    "        return np.maximum(z, 0)\n",
    "    elif fname == 'Sigmoid':\n",
    "        return 1./(1+np.exp(-z))\n",
    "    elif fname == 'Tanh':\n",
    "        return np.tanh(z)\n",
    "    elif fname == 'Linear':\n",
    "        return z\n",
    "    else:\n",
    "        raise ValueError('Unknown non-linear function error')\n",
    "        \n",
    "\n",
    "def df(z, fname = 'ReLU'):\n",
    "    \"\"\"\n",
    "      computes and returns the derivative of the non-linear function of z with respect to z\n",
    "      \n",
    "      z: numpy array of any shape \n",
    "      fname: a string that is name of the non-linearity. Defaults to 'ReLU'. Other valid values are\n",
    "             'Sigmoid', 'Tanh', and 'Linear'.\n",
    "      \n",
    "      returns df/dz where f is the non-linear function of z. Name of the non-linear function is fname.\n",
    "    \"\"\"\n",
    "    if fname == 'ReLU':\n",
    "        return z>0\n",
    "    elif fname == 'Sigmoid':\n",
    "        sigma_z = 1./(1+np.exp(-z))\n",
    "        return sigma_z * (1-sigma_z)\n",
    "    elif fname == 'Tanh':\n",
    "        return 1 - np.tanh(z)**2\n",
    "    elif fname == 'Linear':\n",
    "        return np.ones(z.shape)\n",
    "    else:\n",
    "        raise ValueError('Unknown non-linear function error')\n",
    "        \n",
    "\n",
    "def forward(a, W, b, fname = 'ReLU'):\n",
    "    \"\"\"\n",
    "      Forward propagates a through the current layer given W and b\n",
    "      a: I/p activation from previous activation layer l-1 of shape nx[l-1] x m\n",
    "      w: weight matrix of shape nx[l] x nx[l-1]\n",
    "      b: bias vector of shape nx[l+1] x 1\n",
    "      \n",
    "      returns anew: the output activation from current layer of shape nx[l] x m\n",
    "              cache: a tuple that contains current layer's linear computation z, previous layer's activation a,\n",
    "                     current layer's activation anew and weight matrix W\n",
    "    \"\"\"\n",
    "    # Fill rhs in the following 3 lines. No extra lines of code required.\n",
    "    \n",
    "    z =                        # np.dot or np.matmul or @ operator will be useful. Also understand numpy \n",
    "                               # broadcasting for adding vector b to product of W and a\n",
    "    anew =                    # function f defined above will be useful\n",
    "    cache =                  # read the doc string for this function listed above and acoordingly fill rhs\n",
    "    return anew, cache\n",
    "\n",
    "\n",
    "def backward(da, cache, fname = 'ReLU'):\n",
    "    \"\"\"\n",
    "      Backward propagates da through the current layer given da, cache and the non-linearity at the current layer\n",
    "      da: derivative of loss with respect current layers activation a; shape is nx[l] x m\n",
    "      cache: a tuple that contains current layer's linear computation z, previous layer's activation aprev,\n",
    "                     current layer's activation a and weight matrix W between previous layer l-1 and current layer l\n",
    "      fname: name of the non-linearity at current layer l; this will be helpful for local gradient computation in \n",
    "             chain rule\n",
    "      \n",
    "      returns dW: derivative of loss with respect to W; shape is nx[l] x nx[l-1]\n",
    "              db: derivative of loss with respect to b; shape is nx[l] x 1\n",
    "    \"\"\"\n",
    "    # Fill rhs in the following 5 lines. No extra lines of code required.\n",
    "    \n",
    "    z, aprev, a, W =            # extract from cache\n",
    "    dz =                        # compute dz as incoming grad da * local grad. For local grad, function df defined \n",
    "                                # above will be useful\n",
    "    dW =                       # np.dot or np.amtmul or @ operator will be useful. Also .T will be useful for \n",
    "                              # transposing\n",
    "    db =                     # np.sum will be useful\n",
    "    daprev =                # np.dot or np.amtmul or @ operator will be useful. Also .T will be useful for \n",
    "                           # transposing\n",
    "    return daprev, dW, db\n",
    "\n",
    "def update_params(Wlist, blist, dWlist, dblist, alpha):\n",
    "    \"\"\"\n",
    "      Updates all the parameters using gradient descent rule\n",
    "      \n",
    "      Wlist: a lsit of all weight matrices to be updated\n",
    "      blist: a list of bias vectors to be updated\n",
    "      dWlist: a list of gradients of loss with respect to weight matrices\n",
    "      dblist: a list of gradients of loss with respect to bias vectors\n",
    "      alpha: learning rate\n",
    "    \"\"\"\n",
    "    for i in range(len(Wlist)):\n",
    "        Wlist[i] -=          # fill rhs\n",
    "        blist[i] -=          # fill rhs\n",
    "        \n",
    "def main(): # main function to train the model\n",
    "    \n",
    "    # load train data\n",
    "    a0, y = load_train_data()\n",
    "    a0 = flatten(a0)\n",
    "    a0 = a0/255. # normalize the data to [0, 1]    \n",
    "    \n",
    "    # set some hyperparameters and epsilon\n",
    "    alpha = 0.01    \n",
    "    miter = 2000\n",
    "    epsilon = 1e-6\n",
    "    num_layers = 2\n",
    "    nx = [a0.shape[0], 32, 1]\n",
    "    m = a0.shape[1]\n",
    "    fname_list = ['ReLU', 'Sigmoid']\n",
    "    \n",
    "    # initialize weights and biases\n",
    "    Wlist, blist =       # fill rhs \n",
    "    \n",
    "    # initialize list of caches from each layer, gradients of weights at each layer, gradients of biases at\n",
    "    # each layer to empty\n",
    "    cache, dWlist, dblist = ([None]*num_layers for i in range(3))\n",
    "    \n",
    "    for i in range(miter):\n",
    "        a = a0\n",
    "        # forward propagate through each layer\n",
    "        for l in range(num_layers):\n",
    "            a, cache[l] =                        # Fill rhs. call forward function with \n",
    "                                                # appropriate arguments\n",
    "\n",
    "        L =                                    # Fill rhs. compute loss L\n",
    "        da =                                  # Fill rhs. compute da\n",
    "\n",
    "        # backward propagate through each layer to compute gradients\n",
    "        for l in range(num_layers-1, -1, -1):\n",
    "            da, dWlist[l], dblist[l] =                   # Fill rhs. call backward function with \n",
    "                                                        # appropriate arguments\n",
    "\n",
    "        # update_params\n",
    "        update_params(...)          # Replace ...; call update_params function with appropriate arguments\n",
    "\n",
    "        if not i%100: # print loss every 100 iterations\n",
    "                print(f'Loss at iteration {i}:\\t{np.asscalar(L):.4f}')\n",
    "    \n",
    "    return Wlist, blist\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Wlist, blist = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_list = ['ReLU', 'Sigmoid']\n",
    "num_layers = 2\n",
    "def predict(a, Wlist, blist, fname_list):\n",
    "    for l in range(num_layers):\n",
    "            a, _ = forward(a, Wlist[l], blist[l], fname_list[l])\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[a > 0.5] = 1\n",
    "    return predictions\n",
    "\n",
    "def test_model(a, y, Wlist, blist, fname_list):\n",
    "    predictions = predict(a, Wlist, blist, fname_list)\n",
    "    acc = np.mean(predictions == y)\n",
    "    acc = np.asscalar(acc)\n",
    "    return acc\n",
    "\n",
    "x, y = load_train_data()\n",
    "x = flatten(x)\n",
    "x = x/255. # normalize the data to [0, 1]\n",
    "print(f'train accuracy: {test_model(x, y, Wlist, blist, fname_list) * 100:.2f}%')\n",
    "\n",
    "x, y = load_test_data()\n",
    "x = flatten(x)\n",
    "x = x/255. # normalize the data to [0, 1]\n",
    "print(f'test accuracy: {test_model(x, y, Wlist, blist, fname_list) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "1. Why has the test accuracy not improved with this 2-layer network? Explain.\n",
    "2. How does replacement of ReLU by Sigmoid at the hidden layer affct the model?\n",
    "3. Expand the 2 layer network to, say a 4 layer network of your choice. How does this model compare to logistic regresion and 2-layer network?\n",
    "4. Play with a few learning rates and explain your observations.\n",
    "\n",
    "Note: All questions will be answered in the jupyter notebook only. Wherever code is required, you write and run the code in a code cell. For text, write and render in a markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
