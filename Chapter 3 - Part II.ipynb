{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanatory notes to excerpts from Cynthia Dwork's text on privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3 - Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Composition Theorems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main motto in the rest so the sections is to provide you with tools that will further the progress of research and will help coming up with simple operations that will create new mechanism that have the properties of mechanisms from which it was created. \n",
    "\n",
    "That is the reason this section is named __Composition Theorems__ that is how do you combine two mechanisms to obtain newer and more sophisticated ones or how do two mechanism behave when applied one after the other. \n",
    "\n",
    "We have already seen that $\\epsilon$ and $\\delta$ degrade with repeated computations of the same statistic. And on repeated use of the same query the answer will converge to the true statistic. Here we will discuss only regarding how these parameters change when two mechanism are combined.<br><br>\n",
    "\n",
    "__PROPERTY 1__\n",
    "\n",
    "__Let__ $\\mathscr{M}_1 : N^{\\mathscr{|X|}} \\rightarrow R_1$ be an $(ε_1,0)$-differentially private algorithm, and let $\\mathscr{M}_2 : N^{\\mathscr{|X|}} \\rightarrow R_2$ be an $(ε_2,0)$ -differentially private algorithm. <br> <br>\n",
    "__Then__ their combination, defined to be $\\mathscr{M}_{1,2} : N^{\\mathscr{|X|}} \\rightarrow R_1 × R_2$ by the mapping: $\\mathscr{M}_{1,2}(x) = (\\mathscr{M}_1(x),\\mathscr{M}_2(x))$ is $(ε_1+ε_2,0)$-differentially private.\n",
    "\n",
    "in general \n",
    "\n",
    "__If__ $\\mathscr{M}_i : N^{\\mathscr{|X|}} \\rightarrow R_i$ be an $(ε_i,\\delta_i)$-differentially private algorithm, $i \\in  \\{1,2,\\dots,k\\}$ <br><br>\n",
    "__Then__ $\\mathscr{M}_{[k]} : N^{\\mathscr{|X|}} \\rightarrow \\Pi^{k}_{i=1} R_i, i \\in  \\{1,2,\\dots,k\\} $ given by the mapping: $\\mathscr{M}_{[k]}(x) = (\\mathscr{M}_1(x),\\mathscr{M}_2(x),\\dots,\\mathscr{M}_k(x))$ is $(\\sum^{k}_{i=1}ε_i,\\sum^{k}_{i=1}\\delta_i)$-differentially private"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The alternate definition of Differential Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before we go ahead we will require some more ways to represent what we have learnt till now, our primary change in representation will be to the definition itself it can be noted that change in the way we present our definition may help us see more of the subject in better light so here we go :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider two random variables X and Y, there are many ways we can measure some kind of distance between these two random variables we will show several of them :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__KL Divergence or relative entropy__ : between X and Y taking values from the same domain is defined to be :\n",
    "\n",
    "$$D(X||Y) = E_{y \\sim Y}[ln \\frac{Pr[X=x]}{Pr[Y=x]}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Max Divergence__ : similarly another measure is :\n",
    "\n",
    "$$D_\\infty(X||Y) = \\max_{ S \\subset supp(Y)} [ln \\frac{Pr[X \\in S]}{Pr[Y \\in S]}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Statistical Distance__ : \n",
    "\n",
    "$$\\Delta(X,Y) \\stackrel{\\text{def}}{=} \\max_{S}|Pr[X \\in S] - Pr[Y \\in S]|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those who don't know KL Divergence I would suggest go through the article suggested previously in reading resources [Demystifying KL divergence](https://towardsdatascience.com/demystifying-kl-divergence-7ebe4317ee68), you may also have to understand the previous articles but if you are done by now you might have caught where we are leading for an alternative definition. So here we come :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can define $(\\epsilon,0)$ DP in terms of __max divergence__ as \n",
    "\n",
    "A privacy mechanism $\\mathscr{M}$ is said to be $(\\epsilon,0)$ DP if for any two neighboring databases the $x$ and $y$ we have $D_\\infty(\\mathscr{M}(x) || \\mathscr{M}(y)) \\leq \\epsilon$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similar to this for $(\\epsilon,\\delta)$ DP we will come up with the measure as follows \n",
    "\n",
    "we define $\\delta$ __- approximate max divergence__ as $$D_{\\infty}^{\\delta}(X||Y) = \\max_{ S \\subset supp(Y) : Pr[X\\in S] \\geq \\delta} [ln \\frac{Pr[X \\in S] - \\delta}{Pr[Y \\in S]}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the definition becomes :\n",
    "\n",
    "A privacy mechanism $\\mathscr{M}$ is said to be $(\\epsilon,0)$ DP if for any two neighboring databases the $x$ and $y$ we have $D_{\\infty}^{\\delta}(\\mathscr{M}(x) || \\mathscr{M}(y)) \\leq \\epsilon$ and $D_{\\infty}^{\\delta}(\\mathscr{M}(y) || \\mathscr{M}(x)) \\leq \\epsilon$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better grounds for Privacy and a peek into the Advanced Composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why do we need the new representation of privacy ?\n",
    "\n",
    "We see that by defining privacy as above we can uncover certain new insights as now the privacy is purely in terms of random variables rather than in unknown quantities we can use distance like measures to understand them better the above definition was used to show the various types of definitions that one can come across literature.\n",
    "\n",
    "from the insightful representation given above we strive to formalize two fundamental principles that we would love to have :\n",
    "\n",
    "1. We have noticed that the privacy degrades with repeated queries, can we avoid this or at least can we forecast the amount of privacy leak that will happen from repeated use of different Privacy mechanism in composition to each other ? So that we can have modular construction of more sophisticated mechanisms from simple ones.\n",
    "\n",
    "2. Also if the individual's data is present across different datasets can we somehow measure the privacy loss for that individual that we will incur by using different mechanisms across each of the datasets. This question is fundamental from adversarial attack point of view as the adversary may be able to influence how the datasets are constructed and may also influence the participation of the studied individual in creation of a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here if you notice we are trying to create a foundation for a theory such that despite what the adversary tries to do we will acheive immunity from all of that. We are going so far in saying that the adversary if he wishes he may as well try to influence the individual in whatever way he wants to enter about himself in another database but still he won't be able to make out whether or not the individual in question participated in the data collection of data set for which the adversary is creating the attack. \n",
    "\n",
    "Suppose now if we want to ensure such a high privacy criterion how do we create a mechanism, rather first how do we model a scenario of this magnitude. From here on we create a environment where such mechanisms can be built.<br>\n",
    "In the author's terms \"We want to model a composition where the adversary can adaptively affect the databases input to future mechanisms as well as queries to the mechanisms\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathscr{F}$ be a family of database access mechanisms. For a probabilistic adversary  $\\mathscr{A}$, we consider two experiments, $E_0$ and $E_1$ defined as follows :\n",
    "\n",
    "For $i = 1,\\dots,k$ : <br>\n",
    "1. $\\mathscr{A}$ outputs two adjacent databases $x_i^0$ and $x_i^1$, a mechanism $\\mathscr{M}_i \\in \\mathscr{F}$ and parameters $w_i$ that can be input to the mechanism (example : query to the mechanims, database for the mechanism, etc...)\n",
    "2. $\\mathscr{A}$ receives $y_i \\in_R \\mathscr{M}_i(w_i,x_i^b) $ here $\\mathscr{A}$ tosses coin arbitrarily and chooses one of the databases to get the result this to ensure that the results are equally likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So $\\mathscr{A}$ chooses a mechanism (from the offered mechanisms), two adjacent databases and parameters which can be anything from queries to augmented databases (Remember mechanism only takes a database and returns a privacy preserved object, this can be a result or the database itself with modifications to preserve privacy). \n",
    "\n",
    "With these the adversary receives an output string $\\{y_i\\}$, for $i = {1,\\dots,k}$ since the process is repeated k times. The next mechanism can be chosen based on the output of the previous access. \n",
    "\n",
    "According to us despite what he chooses or does we need the result to be same, if not possible \"atleast as close as possible\". That is the adversary can't tell, given the output of all k mechanism, whether the individual's data was used or not. Now the 'k' mechanisms highlighted here may all be over the same database or each over a different database all together this is also an important point of observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we say $\\mathscr{A}$'s __view__ $\\mathscr{V}$ to be the string of outputs and the coin tosses used for the selection of the database that is $\\mathscr{V} = (r,y1,\\dots,y_k)$  where $r$ is the coin tosses of $\\mathscr{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The k-fold adaptive composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we come finally to the more broad inclusion of definition of privacy in terms of the use of mechanimsm and this will help us establish a very good bound on how much the data of a single individual might be leaked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We say that a family $\\mathscr{F}$ of database mechanisms satisfy $\\epsilon$ DP under *__k-fold adaptive composition__*  if for every adversary $\\mathscr{A}$, we have $D_\\infty(\\mathscr{V}^0||\\mathscr{V}^1) \\leq \\epsilon$ where $\\mathscr{V}^b$ denotes the view of $\\mathscr{A}$ in the *k-fold* experiment above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__\"A Better explanation of view can be presented here\"__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This entire thing stated above is to quote a very beautiful result which shows how we can bound the leak of privacy for an indiviudal who's occurence is in k database access mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Advanced Composition :__\n",
    "\n",
    "Suppose we have k - $(\\epsilon,\\delta)$ DP mechanisms (the value of $\\epsilon, \\delta$ might differ but the result still holds all the same) the adaptive composition of these satisfies $(\\epsilon',k\\delta + \\delta')$ DP where :\n",
    "\n",
    "$$\\epsilon' = \\sqrt{2k\\ln(1/\\delta')\\epsilon} + k\\epsilon(e^\\epsilon - 1),\\ \\ \\ \\ \\ \\ \\ \\ni\\ \\ \\ \\epsilon, \\delta, \\delta' \\geq 0$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
