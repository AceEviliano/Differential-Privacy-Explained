{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanatory notes to excerpts from Cynthia Dwork's text on privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2 - Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formalizing Differential Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Probability Simplex :__\n",
    "Given a discrete set B the probability simplex over B, denoted $\\Delta(B)$ is defined to be :\n",
    "$$\\Delta(B)=\\{x \\in R^{|B|} : x_i \\geq 0,  \\forall i \\ \\ and  \\sum_{i=1}^{|B|} x_i =1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Intuitive idea :__ So what does probability simplex tell ? it tells we want are n-dimensianal vectors such that the entries of the vector is in range 0 and 1 and also the sum of all entries is 1. And this vector has sam dimesion as the size of discrete set B. So the discrete set represents all the possible discrete events example B={A,B,C} we can assign the probability os the three events to be a vector (1/3, 1/3, 1/3) there are many other ways we can assign probabilities to each event, the collection of all such assignments is called a probability simplex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Randomized Algorithm :__\n",
    "\n",
    "A randomized algorithm $\\mathscr{M}$ with domain $A$ and discrete range $B$ is associated with the mapping $M : A \\rightarrow \\Delta(B)$, On input $a \\in A$ the algorithm outputs $\\mathscr{M}(a) = b$ with probability $(M(a))_b$ for each $b \\in B$. The probability space is over the coin flips of algorithm $\\mathscr{M}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Intuitive idea:__ so how do we break this up ? simple let's first consider the action of $M$ alone we see that $M(a)= y , y \\in \\Delta(B)$ that is $M$ takes an input $a$ and returns what is the probability of getting each of the elements in $B$ (as this is a randomized algorithm). Then $\\mathscr{M}$ returns the element in $B$ with that much probability. To understand the last statement that the probability space is over the coin flips of algorithm $\\mathscr{M}$, remember last time in the notes when we spoke of the random bit strings ? we see any randomized algorithm to be constructed by two parts a deterministic algorithm (algo with know output) and a part that adds a random bit string to this output. So any randomized algo. can be seen as taking in a random bit string and an input belonging to the domain, the random bit strings are nothing but coin flips (1/0, true/false) thus we say the probability space is over the coin flips of $\\mathscr{M}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance between databases :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider databases to be collection of records from universe $\\mathscr{X}$. Take a simple database containing records for classification. Now the records will have say $n$ classes. Then we say the histogram of databse in $\\mathscr{X}$ to be the tuple contatining the number of records of each type example : {a,a,b,c,b,a,c,b,a,a} the histogram of this would be (5,3,2) where a repeats 5 times b 3 times and c 2 times.This is formally denoted as $x \\in N^{|\\mathscr{X}|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\mathscr{l}_1$ distance :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define the $\\mathscr{l}_1$ norm of a dataset with histogram $x$ to be $$||x||_1 = \\sum_{i=1}^{|\\mathscr{X}|} x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from this we define the $\\mathscr{l}_1$ distance between two datasets with histogram $x$ and $y$ to be : $$||x-y||_1 = \\sum_{i=1}^{|\\mathscr{X}|} | x_i-y_i | $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us that the distance between two databases is the number of missing rows per category or class.i.e. suppose the 2 histograms of different databases are (10,8) and (11,11) then the distance is 4 between the databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differential Privacy :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A randomized algorithm $\\mathscr{M}$ with domain $N^{|\\mathscr{X}|}$ <br><br>is $(\\epsilon,\\delta)$-differentially private <br><br> if $\\forall S \\subset Range(\\mathscr{M})$ and $\\forall \\ \\ x,y \\in N^{|\\mathscr{X}|}$, $||x-y||_1 \\leq 1$ <br><br> $Pr[\\mathscr{M}(x) \\in S] \\leq e^{\\epsilon}Pr[\\mathscr{M}(y) \\in S] + \\delta $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So finally we come to the definition of Differential Privacy, let's break it (I have already done it) and analyse this : we see that the first statement says the domain of a randomized algorithm is from histogram of a database from the universe $\\mathscr{X}$, so basically we are taking a randomized algorithm,but why do we need a randomized algorithm in the first place, and what is this randomized algorithm for ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First (we actually come from the last question) : here the randomized algorithm is a privacy mechanism, we discussed this in the earlier notes. This may or may not take a query but it for sure takes the database and makes it secure before any inference is drawn from it.<br>\n",
    "Next : why do we need to have only a randomized algorithm won't a deterministic algorithm do ? can't we preserve privacy with deterministic algorithms ? the answer is **NO**. Here is a simple but elegant proof :\n",
    "\n",
    "suppose we have a nontrivial deterministic algorithm ℳ which can preserve privacy then we have $ℳ(x) \\neq ℳ(y)$ where $x,y \\in N^{|\\mathscr{X}|}$, $||x-y||_1 \\leq 1$ \n",
    "\n",
    "since ℳ is deterministic we can have a differencing attack as $ℳ(x)-ℳ(y_i) = c_i$ where $y_i$ is the histogram of the database with row $r_i$ eliminated, we can reconstruct ℳ from mapping $r_i$ to $c_i$ thus, we will have the inidividual's data leaked when we find an $ℳ^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the second statement quantifies the privacy using the concept of epsilon and delta, this kind of quantification is highly favoured in analysis, Cauchy (mathematician) was the first to come up with such a quantification for conitnuity of a function (which was intuitive till then) and the Weistrass generalized and made it popular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  $x,y \\in N^{|\\mathscr{X}|}$, $||x-y||_1 \\leq 1$  describes taking databases that differ only by one entry at most, that is only one entry missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\forall S \\subset Range(\\mathscr{M})$ describes that you take any subset of outcomes from all possible outcomes that the algorithm $\\mathscr{M}$ returns. suppose $\\mathscr{M}$ returns {a,b,c,d} as all possible outcomes for any subset (example {a,b}, {a,c,d}, etc) we need to satisfy something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that something is :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Pr[\\mathscr{M}(x) \\in S] \\leq e^{\\epsilon}Pr[\\mathscr{M}(y) \\in S] + \\delta $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simply implies that the probability that the altered dataset belongs returns a particular outcome should linearly bound the probability that the normal dataset gives the same outcome. That is the altered dataset should not give a higher probability for some outcome to happen. We are saying the two probabilities to be proportional to some constant $e^{\\epsilon}$\n",
    "\n",
    "here the $\\delta$ plays an important role, to understand it let's understand these probabilities, suppose I say privacy leakage is 0.1% for a dataset of 100 rows this means I will leak one individual's information completely.\n",
    "So here the $\\delta$ is within range of 0 and 1 as the other side of inequality is probability. So having non zero $\\delta$ means we are having a minimum leakage of $\\delta$ *100 %.<br>\n",
    "thus, we need to ensure that we bound $\\delta$ to be \"inverse of any polynomial of the size of dataset\" (line as used in the book) that is we need to ensure that we won't leak data of an individual by ensuring this (take time and get this in terms of probability as described above, bounding by inverse of polynomial only ensures the percentage leakage is less than the size of the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Randomized Response :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the definition we will try to quantify the leakage of Randomized response mechanism :\n",
    "\n",
    "Consider the dataset have \"Yes\" and \"No\" as two responses.<br>\n",
    "Here if we take a altered dataset having distance only one from the original dataset, then we will see that the dataset was changed from \"Yes\" to \"No\" at one entry.\n",
    "\n",
    "now the probability that we can obtain a response \"Yes\" when the honest answers is \"Yes\" is 3/4 (we have done this before) <br>\n",
    "similarly we obtain a response \"Yes\" when the honest answers is \"No\" is 1/4 thus,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the formula to be : \n",
    "\n",
    "$$\\frac{Pr[\\mathscr{M}(x) \\in S]}{Pr[\\mathscr{M}(y) \\in S]} \\leq e^{\\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore, 3/4*1/4 = 3 <br>\n",
    "thus we have $e^{\\epsilon} = 3 \\implies \\epsilon = ln(3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence randomized response is (ln(3),0)-differentially private."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Processing does not affect the privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we finally finish this section (I know this has been alreadty quiet hard till here but let's go ahead). This one important result that we need to understand.<br><br>\n",
    "\n",
    "__Differential Privacy is immune to post processing__ i.e. one can't take a result of databse that has gone through privacy mechanism and somehow make it less private. Once a private mechanism is applied and a some $(\\epsilon,\\delta)$ privacy is established you can't reduce the privacy with just doing computations on that result without any additional knowledge of the database. This guarantees that we can take the result and use it for the purpose of analysis further without decreasing the privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To proove this point we will state here an important result which is deep in itself and comes from spectral theory and stochastic processes. \n",
    "\n",
    "Any randomized mapping can be decomposed into a convex combination of deterministic functions. For those who know the theory of stochastic matrices I would point you to this important [stack overflow question](https://math.stackexchange.com/questions/896331/on-the-decomposition-of-stochastic-matrices-as-convex-combinations-of-zero-one-m) which explains the concept so very well for others we will go with relatively simpler analogy and explain this.\n",
    "\n",
    "Consider a process which generates random numbers. You have these numbers as your data, you find a pattern i.e. a function that can map certain set of points not necessarily all, you remove these points mapped by the function then go ahead and create a new dataset from the previous data. Now we claim that we can extract a pattern from this as well. Repeating the process you create a set of functions $\\{f_1, f_2, \\dots,f_n\\}$ which together are able to replicate the random process but the individual functions themselves are deterministic as it gives a fixed output corresponding to an input. This is possible and is prooved in the [Birkhoff–von Neumann theorem](https://en.wikipedia.org/wiki/Doubly_stochastic_matrix#Birkhoff_polytope_and_Birkhoff.E2.80.93von_Neumann_theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Theorem :$\n",
    "Let $\\mathscr{M} : N^{\\mathscr{|X|}} \\rightarrow R $ be a randomized algorithm that is an $(\\epsilon,\\delta)$ - DP and let $ f : R \\rightarrow R^{'}$ be any arbitrary mapping (randomized or not) then the mapping $f \\circ \\mathscr{M} : N^{\\mathscr{|X|}} \\rightarrow R^{'}$ is $(\\epsilon,\\delta)$ DP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$proof :$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have that the covex combination of DP mechanisms are DP.<br>\n",
    "we have the convex combination as if $x$ is a convex combination of $y_1, y_2 , \\dots, y_n$ then $x$ can be written as:\n",
    "$$x = \\alpha_1y_1 + \\alpha_2y_2 + \\dots + \\alpha_ny_n,\\ \\ \\ \\ \\ni \\sum_{i=1}^{n} \\alpha_i = 1$$<br><br>\n",
    "If $m$ is one mechanism and $m'$  is another the result produced by these two being $r$ and $r'$ the convex combination of these results maintains $(\\epsilon,\\delta)$DP\n",
    "\n",
    "We will proove for a deterministic mapping $f$ that $f \\circ \\mathscr{M}$ is $(\\epsilon,\\delta)$ DP and since any randomized mapping is just a convex combination of deterministic mapping the result follows.\n",
    "\n",
    "fix any neighbouring dataset $||x,y||_1 \\leq 1$ and any event $S \\subset R^{'}$ <br>\n",
    "let $T = {r \\in R : f(r) \\in S}$ then we have :\n",
    "\n",
    "$$Pr[f(\\mathscr{M}(x)) \\in S] = Pr[\\mathscr{M}(x) \\in T] $$ as $f$ is deterministic\n",
    "$$\\leq exp(\\epsilon) Pr[\\mathscr{M}(y)\\in T] + \\delta$$ from definition of $\\mathscr{M}$ being DP<br> \n",
    "$$ = exp(\\epsilon) Pr[f(\\mathscr{M}(y)) \\in  S] + \\delta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we have proven the fact"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
